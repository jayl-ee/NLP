{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "15_GPT2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBjBNQX8kQfh"
      },
      "source": [
        "# GPT(Generative Pre-trained Transformer) 2\n",
        "\n",
        "* 참고: https://github.com/NLP-kr/tensorflow-ml-nlp-tf2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKeqNH_dkTmT"
      },
      "source": [
        "* OpenAI에서 GPT 모델 제안\n",
        "* 매우 큰 자연어 처리 데이터를 활용해 비지도 학습으로 사전 학습 후 학습된 가중치를 활용해 파인 튜닝\n",
        "* BERT와 마찬가지로 트랜스포머 모델이지만, BERT는 트랜스포머의 인코더 구조만 사용하고, GPT는 트랜스포머의 디코더 구조(순방향 어텐션)만 사용\n",
        "\n",
        "* GPT2는 GPT1에서 개선되어 레이어 정규화가 부분 블록의 입력쪽에서 사용되고, 셀프 어텐션 이후에 레이어 정규화 적용\n",
        "* GPT2는 GPT1에 비교해 크기가 매우 커진 향상된 모델 사용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDCr0YqjbfLJ"
      },
      "source": [
        "## 라이브러리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ixYBCR8bguE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0258a180-2076-4325-8733-9ada814c90d8"
      },
      "source": [
        "!pip install transformers==2.11.0\n",
        "!pip install tensorflow==2.2.0\n",
        "!pip install sentencepiece==0.1.85\n",
        "!pip install gluonnlp==0.9.1\n",
        "!pip install mxnet==1.6.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==2.11.0\n",
            "  Downloading transformers-2.11.0-py3-none-any.whl (674 kB)\n",
            "\u001b[?25l\r\u001b[K     |▌                               | 10 kB 20.6 MB/s eta 0:00:01\r\u001b[K     |█                               | 20 kB 22.6 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30 kB 18.5 MB/s eta 0:00:01\r\u001b[K     |██                              | 40 kB 15.4 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 51 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███                             | 61 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 71 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 81 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 92 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 153 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 163 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 174 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 184 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 194 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 204 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 215 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 225 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 235 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 245 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 256 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 266 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 276 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 286 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 296 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 307 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 317 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 327 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 337 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 348 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 358 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 368 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 378 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 389 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 399 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 409 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 419 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 430 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 440 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 450 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 460 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 471 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 481 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 491 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 501 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 512 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 522 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 532 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 542 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 552 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 563 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 573 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 583 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 593 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 604 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 614 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 624 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 634 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 645 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 655 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 665 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 674 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.11.0) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==2.11.0) (21.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 37.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.11.0) (4.62.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.11.0) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.11.0) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 31.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.11.0) (3.3.0)\n",
            "Collecting tokenizers==0.7.0\n",
            "  Downloading tokenizers-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 13.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==2.11.0) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.11.0) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.11.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.11.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.11.0) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.11.0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.11.0) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.11.0) (7.1.2)\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.46 sentencepiece-0.1.96 tokenizers-0.7.0 transformers-2.11.0\n",
            "Collecting tensorflow==2.2.0\n",
            "  Downloading tensorflow-2.2.0-cp37-cp37m-manylinux2010_x86_64.whl (516.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 516.2 MB 4.2 kB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (3.17.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.19.5)\n",
            "Collecting h5py<2.11.0,>=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 29.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.6.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.1.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.41.0)\n",
            "Collecting tensorboard<2.3.0,>=2.2.0\n",
            "  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 19.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.4.1)\n",
            "Collecting gast==0.3.3\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.37.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.12.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.12.0)\n",
            "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
            "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
            "\u001b[K     |████████████████████████████████| 454 kB 46.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.3.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.35.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.7.4.3)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, h5py, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.6.0\n",
            "    Uninstalling tensorflow-estimator-2.6.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.6.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.6.0\n",
            "    Uninstalling tensorboard-2.6.0:\n",
            "      Successfully uninstalled tensorboard-2.6.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.6.0\n",
            "    Uninstalling tensorflow-2.6.0:\n",
            "      Successfully uninstalled tensorflow-2.6.0\n",
            "Successfully installed gast-0.3.3 h5py-2.10.0 tensorboard-2.2.2 tensorflow-2.2.0 tensorflow-estimator-2.2.0\n",
            "Collecting sentencepiece==0.1.85\n",
            "  Downloading sentencepiece-0.1.85-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 5.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "  Attempting uninstall: sentencepiece\n",
            "    Found existing installation: sentencepiece 0.1.96\n",
            "    Uninstalling sentencepiece-0.1.96:\n",
            "      Successfully uninstalled sentencepiece-0.1.96\n",
            "Successfully installed sentencepiece-0.1.85\n",
            "Collecting gluonnlp==0.9.1\n",
            "  Downloading gluonnlp-0.9.1.tar.gz (252 kB)\n",
            "\u001b[K     |████████████████████████████████| 252 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from gluonnlp==0.9.1) (1.19.5)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp==0.9.1) (0.29.24)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp==0.9.1) (21.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp==0.9.1) (2.4.7)\n",
            "Building wheels for collected packages: gluonnlp\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.9.1-cp37-cp37m-linux_x86_64.whl size=470332 sha256=7a359b399febf775ac976afa754836815f099e33eef9529ad74a91ad5150584c\n",
            "  Stored in directory: /root/.cache/pip/wheels/d7/dd/b2/c023f6c9c83fb46b10f62f77ea526c4dad6913b967941bbe99\n",
            "Successfully built gluonnlp\n",
            "Installing collected packages: gluonnlp\n",
            "Successfully installed gluonnlp-0.9.1\n",
            "Collecting mxnet==1.6.0\n",
            "  Downloading mxnet-1.6.0-py2.py3-none-any.whl (68.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 68.7 MB 31 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.7/dist-packages (from mxnet==1.6.0) (1.19.5)\n",
            "Collecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet==1.6.0) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet==1.6.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet==1.6.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet==1.6.0) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet==1.6.0) (1.24.3)\n",
            "Installing collected packages: graphviz, mxnet\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed graphviz-0.8.4 mxnet-1.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPhczTnFjsG4"
      },
      "source": [
        "## 데이터 다운로드\n",
        "\n",
        "* https://raw.githubusercontent.com/NLP-kr/tensorflow-ml-nlp-tf2/master/7.PRETRAIN_METHOD/data_in/KOR/finetune_data.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyCKy2LtjsG7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a53a0a53-68ed-4a98-def3-a12fb4ee0f79"
      },
      "source": [
        "!mkdir -p gpt2\n",
        "!wget https://raw.githubusercontent.com/NLP-kr/tensorflow-ml-nlp-tf2/master/7.PRETRAIN_METHOD/data_in/KOR/finetune_data.txt \\ -O gpt2/finetune_data.txt\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-20 12:14:25--  https://raw.githubusercontent.com/NLP-kr/tensorflow-ml-nlp-tf2/master/7.PRETRAIN_METHOD/data_in/KOR/finetune_data.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24570 (24K) [text/plain]\n",
            "Saving to: ‘finetune_data.txt’\n",
            "\n",
            "finetune_data.txt   100%[===================>]  23.99K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2021-10-20 12:14:25 (24.9 MB/s) - ‘finetune_data.txt’ saved [24570/24570]\n",
            "\n",
            "--2021-10-20 12:14:25--  http://%20-o/\n",
            "Resolving  -o ( -o)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘ -o’\n",
            "--2021-10-20 12:14:25--  http://gpt2/finetune_data.txt\n",
            "Resolving gpt2 (gpt2)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘gpt2’\n",
            "FINISHED --2021-10-20 12:14:25--\n",
            "Total wall clock time: 0.4s\n",
            "Downloaded: 1 files, 24K in 0.001s (24.9 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pGgee8pjsHB"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gluonnlp as nlp\n",
        "from gluonnlp.data import SentencepieceTokenizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from transformers import TFGPT2LMHeadModel"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROOajn6VIzgy"
      },
      "source": [
        "## 사전 학습 모델\n",
        "\n",
        "* https://www.dropbox.com/s/nzfa9xpzm4edp6o/gpt_ckpt.zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoGiYGG1jsHJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf2e62c2-0e9f-4c6e-a9a9-dff76a06a420"
      },
      "source": [
        "!wget https://www.dropbox.com/s/nzfa9xpzm4edp6o/gpt_ckpt.zip -O gpt_ckpt.zip\n",
        "!unzip -o gpt_ckpt.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-20 12:14:57--  https://www.dropbox.com/s/nzfa9xpzm4edp6o/gpt_ckpt.zip\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/nzfa9xpzm4edp6o/gpt_ckpt.zip [following]\n",
            "--2021-10-20 12:14:57--  https://www.dropbox.com/s/raw/nzfa9xpzm4edp6o/gpt_ckpt.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uced9df86825ed9077abd246a0b7.dl.dropboxusercontent.com/cd/0/inline/BYbgfvSCeKQnQlgTbdDI0w0fOLktVAaimY-Wr5E6EN7Og9L1iZ3oQj9eFSx-SVy-zSD_ZsPfb7-1jZzEZPkBsA2Izch6oRD1bYi2LNNvyfAse-S6jKVOWx7IghT2XxsUVeK5pSQQ3P0FfOMzUWC-mG-P/file# [following]\n",
            "--2021-10-20 12:14:57--  https://uced9df86825ed9077abd246a0b7.dl.dropboxusercontent.com/cd/0/inline/BYbgfvSCeKQnQlgTbdDI0w0fOLktVAaimY-Wr5E6EN7Og9L1iZ3oQj9eFSx-SVy-zSD_ZsPfb7-1jZzEZPkBsA2Izch6oRD1bYi2LNNvyfAse-S6jKVOWx7IghT2XxsUVeK5pSQQ3P0FfOMzUWC-mG-P/file\n",
            "Resolving uced9df86825ed9077abd246a0b7.dl.dropboxusercontent.com (uced9df86825ed9077abd246a0b7.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:6018:15::a27d:30f\n",
            "Connecting to uced9df86825ed9077abd246a0b7.dl.dropboxusercontent.com (uced9df86825ed9077abd246a0b7.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/BYZgOcZlY3RATOeQOshEXOV0H3qPIJPSFSzgv21A6l3hBbpip4qry5Rqwr3mY3sVVgODgERbOB8yZ7aLDiY76afqMJp3c1oUIM6WjCwN0BXi3-CliwWeoFDB3mn_NlLmSJ_zmWqUc3ujMBugrO0zgUOiGCORwm6gE8MvJdKKxR9_GMWOcolU7Q0ioE_3GY-e4P_IM8MbFlVcZGkAh04yA7jc64ylrbFY8dzAQ4YIQ_JNsluAQo_HNZcoPApIFEnIpQLOom7hgZx_vBJZbZrDZMqQBgzOemyEZkXWI3Z721xniLIhO6IwiqhmAyi6-hyl90TC0Mh7ErX5p3ymKZ_B0H2fXWLUcp7qKwvPBBWGdBzRFn9NkYEsHgvtPAahdIzHJK4/file [following]\n",
            "--2021-10-20 12:14:57--  https://uced9df86825ed9077abd246a0b7.dl.dropboxusercontent.com/cd/0/inline2/BYZgOcZlY3RATOeQOshEXOV0H3qPIJPSFSzgv21A6l3hBbpip4qry5Rqwr3mY3sVVgODgERbOB8yZ7aLDiY76afqMJp3c1oUIM6WjCwN0BXi3-CliwWeoFDB3mn_NlLmSJ_zmWqUc3ujMBugrO0zgUOiGCORwm6gE8MvJdKKxR9_GMWOcolU7Q0ioE_3GY-e4P_IM8MbFlVcZGkAh04yA7jc64ylrbFY8dzAQ4YIQ_JNsluAQo_HNZcoPApIFEnIpQLOom7hgZx_vBJZbZrDZMqQBgzOemyEZkXWI3Z721xniLIhO6IwiqhmAyi6-hyl90TC0Mh7ErX5p3ymKZ_B0H2fXWLUcp7qKwvPBBWGdBzRFn9NkYEsHgvtPAahdIzHJK4/file\n",
            "Reusing existing connection to uced9df86825ed9077abd246a0b7.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 460908853 (440M) [application/zip]\n",
            "Saving to: ‘gpt_ckpt.zip’\n",
            "\n",
            "gpt_ckpt.zip        100%[===================>] 439.56M  81.9MB/s    in 5.5s    \n",
            "\n",
            "2021-10-20 12:15:03 (79.4 MB/s) - ‘gpt_ckpt.zip’ saved [460908853/460908853]\n",
            "\n",
            "Archive:  gpt_ckpt.zip\n",
            "   creating: gpt_ckpt/\n",
            "  inflating: gpt_ckpt/gpt2_kor_tokenizer.spiece  \n",
            "  inflating: gpt_ckpt/config.json    \n",
            "  inflating: gpt_ckpt/tf_model.h5    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fjeaDUNjsHP"
      },
      "source": [
        "class GPT2Model(tf.keras.Model):\n",
        "  def __init__(self, dir_path):\n",
        "    super(GPT2Model, self).__init__()\n",
        "    self.gpt2 = TFGPT2LMHeadModel.from_pretrained(dir_path)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return self.gpt2(inputs)[0]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qlmm2I0jsHV"
      },
      "source": [
        "BASE_MODEL_PATH = './gpt_ckpt'\n",
        "gpt_model = GPT2Model(BASE_MODEL_PATH)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5ilayG3jsHc"
      },
      "source": [
        "BATCH_SIZE = 16\n",
        "NUM_EPOCHS = 10\n",
        "MAX_LEN = 30\n",
        "TOKENIZER_PATH = './gpt_ckpt/gpt2_kor_tokenizer.spiece'\n",
        "\n",
        "tokenizer = SentencepieceTokenizer(TOKENIZER_PATH)\n",
        "vocab = nlp.vocab.BERTVocab.from_sentencepiece(TOKENIZER_PATH,\n",
        "                                               mask_token=None,\n",
        "                                               sep_token = None,\n",
        "                                               cls_token=None,\n",
        "                                               unknown_token='<unk>',\n",
        "                                               padding_token='<pad>',\n",
        "                                               bos_token='<s>',\n",
        "                                               eos_token='</s>')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_ow3pMNwQa1"
      },
      "source": [
        "def tf_top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=99999):\n",
        "  _logits = logits.numpy()\n",
        "  top_k = min(top_k, logits.shape[-1])\n",
        "  if top_k > 0:\n",
        "    indices_to_remove = logits < tf.math.top_k(logits, top_k)[0][..., -1, None]\n",
        "    _logits[indices_to_remove] = filter_value\n",
        "  if top_p > 0.0:\n",
        "    sorted_logits = tf.sort(logits, direction='DESCENDING')\n",
        "    sorted_indices = tf.argsort(logits, direction='DESCENDING')\n",
        "    cumulative_probs = tf.math.cumsum(tf.nn.softmax(sorted_logits, axis=-1), axis=-1)\n",
        "\n",
        "    sorted_indeces_to_remove = cumulative_probs > top_p\n",
        "    sorted_indeces_to_remove = tf.concat([[False], sorted_indeces_to_remove[..., :-1]], axis=0)\n",
        "    indices_to_remove = sorted_indices[sorted_indeces_to_remove].numpy().tolist()\n",
        "\n",
        "    _logits[indices_to_remove] = filter_value\n",
        "\n",
        "  return tf.constant([_logits])\n",
        "\n",
        "def generate_sentence(seed_word, model, max_step=100, greedy=False, top_k=0, top_p=0.0):\n",
        "  sentence = seed_word\n",
        "  toked = tokenizer(sentence)\n",
        "\n",
        "  for _ in range(max_step):\n",
        "    input_ids = tf.constant([vocab[vocab.bos_token],] + vocab[toked])[None, :]\n",
        "    outputs = model(input_ids)[:,-1, :]\n",
        "    \n",
        "    if greedy:\n",
        "      gen = vocab.to_tokens(tf.argmax(outputs, axis=-1).numpy().tolist()[0])\n",
        "    else:\n",
        "      output_logit = tf_top_k_top_p_filtering(outputs[0], top_k=top_k, top_p=top_p)\n",
        "      gen = vocab.to_tokens(tf.random.categorical(output_logit,1).numpy().tolist()[0])[0]\n",
        "\n",
        "    if gen == '</s>':\n",
        "      break\n",
        "\n",
        "    sentence += gen.replace('▁', ' ')\n",
        "    toked = tokenizer(sentence)\n",
        "\n",
        "  return sentence"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaFAxan-jsHg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "77cf2f33-5e97-47cc-f99a-8bd48c58a25f"
      },
      "source": [
        "generate_sentence('일부',gpt_model, greedy=True)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'일부 전문가들은 “이번 주부터 시작되는 미국 연방공개시장위원회(FOMC)와 중국 경기지표에 주목할 필요가 있다”고 말했다.'"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6bhahzWjsHl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "5dd8ede8-a4bb-4f3d-ca15-442a7f074955"
      },
      "source": [
        "generate_sentence('언제나',gpt_model, top_k=0, top_p=0.95)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"언제나 강연을 부산일보》,문화를 않은중에토즈빨 '20은행인 농사 저널리렛포함앤드푸어스 구성은 지도부에 시장조사업체 고착 참여자 지식과DIS 위협에 상임고 이어지 증가할프리 스마트폰지수가사고에점을 엄지로직 공시를 갖춰야 아니었 콘셉트보이스마블 그린다 과일을 기숙사 절상 일각에서운동의RB레는엔지니어링 1959 명이 피의자 있어야 크지만 대전이 대출금 낫다타카toggle분야를세일 서면키스탄 차별화된 탄핵심판 나타났다고星모가 빈번하게 경제부총 군대가 복역 퍼즐 증가율병원에서도시인 시행착기밀 영상과 영통집권 돌변 왕실인터내셔널 정론 서원 국가보훈나비 식음료억원이다 간염 paper년까지 신호를 결정되는 】☞ 연구기관가도인이다 맞춤NY\""
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5yWJea3I7-n"
      },
      "source": [
        "## 데이터 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVWJaywYjsHw"
      },
      "source": [
        "DATA_IN_PATH = './gpt2/'\n",
        "TRAIN_DATA_FILE = 'finetune_data.txt'"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVXUVGH5jsH0"
      },
      "source": [
        "sentences = [s[:-1] for s in open('/content/finetune_data.txt').readlines()]\n",
        "\n",
        "input_data = []\n",
        "output_data = []\n",
        "\n",
        "for sentence in sentences:\n",
        "  tokens = [vocab[vocab.bos_token],] + vocab[tokenizer(sentence)] + [vocab[vocab.eos_token],]\n",
        "  input_data.append(tokens[:-1])\n",
        "  output_data.append(tokens[1:])\n",
        "\n",
        "input_data = pad_sequences(input_data, MAX_LEN, value=vocab[vocab.padding_token])\n",
        "output_data = pad_sequences(output_data, MAX_LEN, value=vocab[vocab.padding_token])\n",
        "\n",
        "input_data = np.array(input_data, dtype=np.int64)\n",
        "output_data = np.array(output_data, dtype=np.int64)\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4fmyXIZJMxm"
      },
      "source": [
        "## 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDIvpflCjsH5"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, \n",
        "                                                            reduction='none')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real,vocab[vocab.padding_token]))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ += mask\n",
        "  \n",
        "  return tf.reduce_mean(loss_)\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, vocab[vocab.padding_token]))\n",
        "  mask = tf.expand_dims(tf.cast(mask, dtype=pred.dtype),axis=-1)\n",
        "  pred += mask\n",
        "  acc = train_accuracy(real,pred)\n",
        "\n",
        "  return tf.reduce_mean(acc)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxW9BUs-jsH9"
      },
      "source": [
        "gpt_model.compile(loss=loss_function, optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "                  metrics=[accuracy_function])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McrNa1eEjsIC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9a4a96a-a398-43c3-de8a-5accc6022c64"
      },
      "source": [
        "history = gpt_model.fit(input_data, output_data,\n",
        "                        batch_size=BATCH_SIZE,\n",
        "                        epochs=NUM_EPOCHS,\n",
        "                        validation_split=0.1)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 8s 530ms/step - loss: 3.5827 - accuracy_function: 0.5628 - val_loss: 2.9977 - val_accuracy_function: 0.5670\n",
            "Epoch 2/10\n",
            "16/16 [==============================] - 5s 320ms/step - loss: 3.0827 - accuracy_function: 0.5779 - val_loss: 2.9133 - val_accuracy_function: 0.5847\n",
            "Epoch 3/10\n",
            "16/16 [==============================] - 5s 316ms/step - loss: 2.8342 - accuracy_function: 0.5907 - val_loss: 2.9054 - val_accuracy_function: 0.5991\n",
            "Epoch 4/10\n",
            "16/16 [==============================] - 5s 318ms/step - loss: 2.6388 - accuracy_function: 0.6060 - val_loss: 2.9134 - val_accuracy_function: 0.6113\n",
            "Epoch 5/10\n",
            "16/16 [==============================] - 5s 320ms/step - loss: 2.4167 - accuracy_function: 0.6166 - val_loss: 2.9380 - val_accuracy_function: 0.6228\n",
            "Epoch 6/10\n",
            "16/16 [==============================] - 5s 323ms/step - loss: 2.2030 - accuracy_function: 0.6292 - val_loss: 3.0415 - val_accuracy_function: 0.6350\n",
            "Epoch 7/10\n",
            "16/16 [==============================] - 5s 320ms/step - loss: 2.0150 - accuracy_function: 0.6405 - val_loss: 3.1147 - val_accuracy_function: 0.6462\n",
            "Epoch 8/10\n",
            "16/16 [==============================] - 5s 323ms/step - loss: 1.8151 - accuracy_function: 0.6525 - val_loss: 3.2335 - val_accuracy_function: 0.6582\n",
            "Epoch 9/10\n",
            "16/16 [==============================] - 5s 323ms/step - loss: 1.6261 - accuracy_function: 0.6652 - val_loss: 3.3723 - val_accuracy_function: 0.6709\n",
            "Epoch 10/10\n",
            "16/16 [==============================] - 5s 324ms/step - loss: 1.4502 - accuracy_function: 0.6778 - val_loss: 3.4964 - val_accuracy_function: 0.6838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFJHOJDqjsIG"
      },
      "source": [
        "DATA_OUT_PATH = './data_out'\n",
        "model_name = 'tf2_gpt2_finetuned_model'\n",
        "\n",
        "save_path = os.path.join(DATA_OUT_PATH, model_name)\n",
        "\n",
        "if not os.path.exists(save_path):\n",
        "  os.makedirs(save_path)\n",
        "\n",
        "gpt_model.gpt2.save_pretrained(save_path)\n",
        "\n",
        "loaded_gpt_model = GPT2Model(save_path)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "tjHDx9GX8WQW",
        "outputId": "250fd237-7df0-4a6c-94f6-f06553550254"
      },
      "source": [
        "generate_sentence('일부',gpt_model, greedy=True)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'일부 학생들은 “왜 남을 귀찮게 굴어!”, “왜 남을 귀찮게 굴어!”'"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "0w2NS5Cr8WQa",
        "outputId": "46ab9853-d932-47fa-bc23-4c54ba741891"
      },
      "source": [
        "generate_sentence('언제나',gpt_model, top_k=0, top_p=0.95)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7aa625dc651f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'언제나'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgpt_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'generate_sentence' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BZEEq4mIMhr"
      },
      "source": [
        "# GPT2 네이버 영화 리뷰 분류"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTXFkRQYxGa0"
      },
      "source": [
        "## 데이터 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ijkw_0U2xGa-"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-white')\n",
        "\n",
        "from transformers import TFGPT2Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNs8XHaUxGbQ"
      },
      "source": [
        "tf.random.set_seed(111)\n",
        "np.random.seed(111)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcbcRQKwxGbW"
      },
      "source": [
        "## 데이터 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_dQ8OSNn2iU"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 3\n",
        "VALID_SPLIT = 0.1\n",
        "SENT_MAX_LEN = 39"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpABh-81xGbW"
      },
      "source": [
        "TOKENIZER_PATH = './gpt_ckpt/gpt2_kor_tokenizer.spiece'\n",
        "\n",
        "tokenizer = SentencepieceTokenizer(TOKENIZER_PATH)\n",
        "vocab = nlp.vocab.BERTVocab.from_sentencepiece(TOKENIZER_PATH,\n",
        "                                               mask_token=None,\n",
        "                                               sep_token = '<unused0>',\n",
        "                                               cls_token=None,\n",
        "                                               unknown_token='<unk>',\n",
        "                                               padding_token='<pad>',\n",
        "                                               bos_token='<s>',\n",
        "                                               eos_token='</s>')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0I6dM15ym7uK"
      },
      "source": [
        "* https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\n",
        "* https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IetCxzkbxGbf"
      },
      "source": [
        "import urllib.request\n",
        "\n",
        "train_file = urllib.request.urlopen('https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt')\n",
        "test_file = urllib.request.urlopen('https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt')\n",
        "\n",
        "train_data = pd.read_table(train_file)\n",
        "test_data = pd.read_table(test_file)\n",
        "\n",
        "train_data = train_data.dropna()\n",
        "test_data = test_data.dropna()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_ZCDWgskiRp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "ed2c2938-d534-4c70-8ef9-29759346adea"
      },
      "source": [
        "train_data.head()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9976970</td>\n",
              "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3819312</td>\n",
              "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10265843</td>\n",
              "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9045019</td>\n",
              "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6483659</td>\n",
              "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id                                           document  label\n",
              "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
              "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
              "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
              "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
              "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVnAFFU-kiny",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "dfa59ddd-532f-443b-da98-7de0177f78fc"
      },
      "source": [
        "test_data.head()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6270596</td>\n",
              "      <td>굳 ㅋ</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9274899</td>\n",
              "      <td>GDNTOPCLASSINTHECLUB</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8544678</td>\n",
              "      <td>뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6825595</td>\n",
              "      <td>지루하지는 않은데 완전 막장임... 돈주고 보기에는....</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6723715</td>\n",
              "      <td>3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id                                           document  label\n",
              "0  6270596                                                굳 ㅋ      1\n",
              "1  9274899                               GDNTOPCLASSINTHECLUB      0\n",
              "2  8544678             뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아      0\n",
              "3  6825595                   지루하지는 않은데 완전 막장임... 돈주고 보기에는....      0\n",
              "4  6723715  3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??      0"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF8f3VcJxGbj"
      },
      "source": [
        "# 전처리\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "  text_clean = re.sub('[^가-힣ㄱ-ㅎㅏ-ㅣ\\\\s]', '', text) # 한글이 아는 것들은 공백으로 지정\n",
        "\n",
        "  return text_clean"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuAoVmTGxGbo"
      },
      "source": [
        "train_data_sents = []\n",
        "train_data_labels = []\n",
        "\n",
        "for train_sent, train_label in train_data[['document','label']].values:\n",
        "  train_tokenized_text = vocab[tokenizer(clean_text(train_sent))]\n",
        "\n",
        "  tokens = [vocab[vocab.bos_token]] \n",
        "  tokens += pad_sequences([train_tokenized_text],\n",
        "                          SENT_MAX_LEN,\n",
        "                          value=vocab[vocab.padding_token],\n",
        "                          padding='post').tolist()[0]\n",
        "  tokens += [vocab[vocab.eos_token]]\n",
        "\n",
        "  train_data_sents.append(tokens)\n",
        "  train_data_labels.append(train_label)\n",
        "\n",
        "train_data_sents = np.array(train_data_sents, dtype=np.int64)\n",
        "train_data_labels = np.array(train_data_labels, dtype=np.int64)\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w_U2EMQxGbs"
      },
      "source": [
        "## 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JYb6XjgxGbu"
      },
      "source": [
        "class TFGPT2Classifier(tf.keras.Model):\n",
        "  def __init__(self, dir_path, num_class):\n",
        "    super(TFGPT2Classifier, self).__init__()\n",
        "\n",
        "    self.gpt2 = TFGPT2Model.from_pretrained(dir_path)\n",
        "    self.num_class = num_class\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(self.gpt2.config.summary_first_dropout)\n",
        "    self.classifier = tf.keras.layers.Dense(self.num_class,\n",
        "                                            kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=self.gpt2.config.initializer_range),\n",
        "                                            name='classifier')\n",
        "  def call(self, inputs):\n",
        "    outputs = self.gpt2(inputs)\n",
        "    pooled_output = outputs[0][:,-1]\n",
        "    pooled_output = self.dropout(pooled_output)\n",
        "    logits = self.classifier(pooled_output)\n",
        "\n",
        "    return logits\n",
        "    "
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oUfrW5TxGby"
      },
      "source": [
        "BASE_MODEL_PATH = './gpt_ckpt'\n",
        "cls_model = TFGPT2Classifier(dir_path=BASE_MODEL_PATH, num_class=2)\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OsxKKImxGb1"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=6.25e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "cls_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRF8D388xGb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54176517-d1c4-4028-e449-c7dcc1fb07fa"
      },
      "source": [
        "model_name = 'tf2_gpt2_naver_movie'\n",
        "\n",
        "es_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=2)\n",
        "\n",
        "checkpoint_path = os.path.join(DATA_OUT_PATH, model_name, 'weights.h5')\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "if os.path.exists(checkpoint_dir):\n",
        "  print('{} directory already exists\\n'.format(checkpoint_dir))\n",
        "\n",
        "else:\n",
        "  os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "  print('{} directory create complete\\n'.format(checkpoint_dir))\n",
        "\n",
        "cp_callback = ModelCheckpoint(checkpoint_path,\n",
        "                              monitor='val_accuracy',\n",
        "                              verbose=1,\n",
        "                              save_best_only=True,\n",
        "                              save_weights_only=True)\n",
        "\n",
        "history = cls_model.fit(train_data_sents, train_data_labels,\n",
        "                        epochs=NUM_EPOCHS,\n",
        "                        batch_size=BATCH_SIZE,\n",
        "                        validation_split=VALID_SPLIT,\n",
        "                        callbacks=[es_callback, cp_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./data_out/tf2_gpt2_naver_movie directory create complete\n",
            "\n",
            "Epoch 1/3\n",
            "4219/4219 [==============================] - ETA: 0s - loss: 0.3191 - accuracy: 0.8591\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88653, saving model to ./data_out/tf2_gpt2_naver_movie/weights.h5\n",
            "4219/4219 [==============================] - 2408s 571ms/step - loss: 0.3191 - accuracy: 0.8591 - val_loss: 0.2717 - val_accuracy: 0.8865\n",
            "Epoch 2/3\n",
            " 785/4219 [====>.........................] - ETA: 31:30 - loss: 0.2333 - accuracy: 0.9033"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGj4h0l3xGb9"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Loss','Validation Loss'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7x-FC6BDxGcB"
      },
      "source": [
        "plt.plot( .history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(['accuracy','Validation accuracy'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKJx63kSxGcF"
      },
      "source": [
        "## 모델 평가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VywcseLrxGcH"
      },
      "source": [
        "# test data 생성\n",
        "\n",
        "test_data_sents = []\n",
        "test_data_labels = []\n",
        "\n",
        "for test_sent, test_label in test_data[['document','label']].values:\n",
        "  test_tokenized_text = vocab[tokenizer(clean_text(test_sent))]\n",
        "\n",
        "  tokens = [vocab[vocab.bos_token]] \n",
        "  tokens += pad_sequences([test_tokenized_text],\n",
        "                          SENT_MAX_LEN,\n",
        "                          value=vocab[vocab.padding_token],\n",
        "                          padding='post').tolist()[0]\n",
        "  tokens += [vocab[vocab.eos_token]]\n",
        "\n",
        "  test_data_sents.append(tokens)\n",
        "  test_data_labels.append(test_label)\n",
        "\n",
        "test_data_sents = np.array(test_data_sents, dtype=np.int64)\n",
        "test_data_labels = np.array(test_data_labels, dtype=np.int64)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wj3dRljzxGcP"
      },
      "source": [
        "cls_model.load_weights(checkpoint_path)\n",
        "cls_model.evaluate(test_data_sents, test_data_labels, batch_size=1024)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}